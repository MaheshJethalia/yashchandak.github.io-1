<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta http-equiv="content-type" content="text/html; charset=utf-8" />

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1" />

  <title>
    
      Publications &middot; Yash Chandak
    
  </title>

  


  <!-- CSS -->
  <link rel="stylesheet" href="/assets/css/main.css" />
  

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Abril+Fatface" />

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/favicon.png" />
<link rel="shortcut icon" href="/favicons/favicon.ico" />

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/feed.xml" />

  <!-- Additional head bits without overriding original head -->


  <!-- Mathjax Support -->
  <script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>

</head>


  <body class="page">

    <div id="sidebar">
  <center>

  <header>
    <img src="/images/dp.jpg" alt="" width="75%" align="center" > 
    <div class="site-title">
      <a href="/">
        
          <span class="back-arrow icon"><svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
  <path d="M0 0h24v24H0z" fill="none"/>
  <path d="M20 11H7.83l5.59-5.59L12 4l-8 8 8 8 1.41-1.41L7.83 13H20v-2z"/>
</svg></span>
        
        Yash Chandak
      </a>
    </div>
    <p class="lead">y[lastname]@cs.umass.edu</p>

  </header>

  <p>
    <br>


  <nav id="sidebar-nav-links">
  
    <a class="home-link "
        href="/">Home</a>
  

  

  


  
    
  

  
    
  

  
    
  

  

  
    
  

  

  
    
  

  
    
  

  
    
  

  

  


  
    
  

  
    
  

  

  
    
  

  

  
    
  

  
    
  

  

  
    
      <a class="page-link  active"
          href="/publication/">Publications</a>
    
  

  
    
      <a class="page-link "
          href="/about/">About Me</a>
    
  


  
    <a class="page-link "
	href="/blog/">Blog</a>
  

  <!-- Optional additional links to insert in sidebar nav -->
<!-- <a href="/docs/resume.pdf">CV/Resume</a> -->

</nav>


  <p>
    <br>
  </p>

  <!-- 
    <span class="site-version">Currently v3.4.1</span>
   -->

  <center>
<nav style="list-style: none; margin: auto; width: 85%; justify-content: center;" id="sidebar-icon-links">
  
    <a id="github-link"
       class="icon" title="Github Project" aria-label="Github Project"
       href="https://github.com/yashchandak" >
      <svg version="1.1" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 28"><path d="M12 2c6.625 0 12 5.375 12 12 0 5.297-3.437 9.797-8.203 11.391-0.609 0.109-0.828-0.266-0.828-0.578 0-0.391 0.016-1.687 0.016-3.297 0-1.125-0.375-1.844-0.812-2.219 2.672-0.297 5.484-1.313 5.484-5.922 0-1.313-0.469-2.375-1.234-3.219 0.125-0.313 0.531-1.531-0.125-3.187-1-0.313-3.297 1.234-3.297 1.234-0.953-0.266-1.984-0.406-3-0.406s-2.047 0.141-3 0.406c0 0-2.297-1.547-3.297-1.234-0.656 1.656-0.25 2.875-0.125 3.187-0.766 0.844-1.234 1.906-1.234 3.219 0 4.594 2.797 5.625 5.469 5.922-0.344 0.313-0.656 0.844-0.766 1.609-0.688 0.313-2.438 0.844-3.484-1-0.656-1.141-1.844-1.234-1.844-1.234-1.172-0.016-0.078 0.734-0.078 0.734 0.781 0.359 1.328 1.75 1.328 1.75 0.703 2.141 4.047 1.422 4.047 1.422 0 1 0.016 1.937 0.016 2.234 0 0.313-0.219 0.688-0.828 0.578-4.766-1.594-8.203-6.094-8.203-11.391 0-6.625 5.375-12 12-12zM4.547 19.234c0.031-0.063-0.016-0.141-0.109-0.187-0.094-0.031-0.172-0.016-0.203 0.031-0.031 0.063 0.016 0.141 0.109 0.187 0.078 0.047 0.172 0.031 0.203-0.031zM5.031 19.766c0.063-0.047 0.047-0.156-0.031-0.25-0.078-0.078-0.187-0.109-0.25-0.047-0.063 0.047-0.047 0.156 0.031 0.25 0.078 0.078 0.187 0.109 0.25 0.047zM5.5 20.469c0.078-0.063 0.078-0.187 0-0.297-0.063-0.109-0.187-0.156-0.266-0.094-0.078 0.047-0.078 0.172 0 0.281s0.203 0.156 0.266 0.109zM6.156 21.125c0.063-0.063 0.031-0.203-0.063-0.297-0.109-0.109-0.25-0.125-0.313-0.047-0.078 0.063-0.047 0.203 0.063 0.297 0.109 0.109 0.25 0.125 0.313 0.047zM7.047 21.516c0.031-0.094-0.063-0.203-0.203-0.25-0.125-0.031-0.266 0.016-0.297 0.109s0.063 0.203 0.203 0.234c0.125 0.047 0.266 0 0.297-0.094zM8.031 21.594c0-0.109-0.125-0.187-0.266-0.172-0.141 0-0.25 0.078-0.25 0.172 0 0.109 0.109 0.187 0.266 0.172 0.141 0 0.25-0.078 0.25-0.172zM8.937 21.438c-0.016-0.094-0.141-0.156-0.281-0.141-0.141 0.031-0.234 0.125-0.219 0.234 0.016 0.094 0.141 0.156 0.281 0.125s0.234-0.125 0.219-0.219z"></path>
</svg>

    </a>
    <!-- <a id="github-download-link"
       class="icon" title="Download" aria-label="Download"
       href="https://github.com/yashchandak/archive/v3.4.1.zip">
      <svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
    <path d="M19 9h-4V3H9v6H5l7 7 7-7zM5 18v2h14v-2H5z"/>
    <path d="M0 0h24v24H0z" fill="none"/>
</svg>
    </a> -->
  

  
    <a id="twitter-link"
       class="icon" title="Twitter" aria-label="Twitter"
       href="https://twitter.com/chandakyash13">
      <?xml version="1.0" ?><!DOCTYPE svg  PUBLIC '-//W3C//DTD SVG 1.1//EN'  'http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd'><svg enable-background="new 0 0 56.693 56.693" height="56.693px" id="Layer_1" version="1.1" viewBox="0 0 56.693 56.693" width="56.693px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M28.348,5.157c-13.6,0-24.625,11.027-24.625,24.625c0,13.6,11.025,24.623,24.625,24.623c13.6,0,24.623-11.023,24.623-24.623  C52.971,16.184,41.947,5.157,28.348,5.157z M40.752,24.817c0.013,0.266,0.018,0.533,0.018,0.803c0,8.201-6.242,17.656-17.656,17.656  c-3.504,0-6.767-1.027-9.513-2.787c0.486,0.057,0.979,0.086,1.48,0.086c2.908,0,5.584-0.992,7.707-2.656  c-2.715-0.051-5.006-1.846-5.796-4.311c0.378,0.074,0.767,0.111,1.167,0.111c0.566,0,1.114-0.074,1.635-0.217  c-2.84-0.57-4.979-3.08-4.979-6.084c0-0.027,0-0.053,0.001-0.08c0.836,0.465,1.793,0.744,2.811,0.777  c-1.666-1.115-2.761-3.012-2.761-5.166c0-1.137,0.306-2.204,0.84-3.12c3.061,3.754,7.634,6.225,12.792,6.483  c-0.106-0.453-0.161-0.928-0.161-1.414c0-3.426,2.778-6.205,6.206-6.205c1.785,0,3.397,0.754,4.529,1.959  c1.414-0.277,2.742-0.795,3.941-1.506c-0.465,1.45-1.448,2.666-2.73,3.433c1.257-0.15,2.453-0.484,3.565-0.977  C43.018,22.849,41.965,23.942,40.752,24.817z"/></svg>





    </a>
  

   
    <a id="linkedin-link"
       class="icon" title="linkedin" aria-label="Linkedin"
       href="https://www.linkedin.com/in/yashchandak/">
      <?xml version="1.0" ?><!DOCTYPE svg  PUBLIC '-//W3C//DTD SVG 1.1//EN'  'http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd'><svg enable-background="new 0 0 56.693 56.693" height="56.693px" id="Layer_1" version="1.1" viewBox="0 0 56.693 56.693" width="56.693px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><g><path d="M30.071,27.101v-0.077c-0.016,0.026-0.033,0.052-0.05,0.077H30.071z"/><path d="M49.265,4.667H7.145c-2.016,0-3.651,1.596-3.651,3.563v42.613c0,1.966,1.635,3.562,3.651,3.562h42.12   c2.019,0,3.654-1.597,3.654-3.562V8.23C52.919,6.262,51.283,4.667,49.265,4.667z M18.475,46.304h-7.465V23.845h7.465V46.304z    M14.743,20.777h-0.05c-2.504,0-4.124-1.725-4.124-3.88c0-2.203,1.67-3.88,4.223-3.88c2.554,0,4.125,1.677,4.175,3.88   C18.967,19.052,17.345,20.777,14.743,20.777z M45.394,46.304h-7.465V34.286c0-3.018-1.08-5.078-3.781-5.078   c-2.062,0-3.29,1.389-3.831,2.731c-0.197,0.479-0.245,1.149-0.245,1.821v12.543h-7.465c0,0,0.098-20.354,0-22.459h7.465v3.179   c0.992-1.53,2.766-3.709,6.729-3.709c4.911,0,8.594,3.211,8.594,10.11V46.304z"/></g></svg>
    </a>
  

  
    <a id="facebook-link"
       class="icon" title="facebook" aria-label="Facebook"
       href="https://www.facebook.com/yashchandak.13">
      <?xml version="1.0" ?><!DOCTYPE svg  PUBLIC '-//W3C//DTD SVG 1.1//EN'  'http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd'><svg enable-background="new 0 0 56.693 56.693" height="56.693px" id="Layer_1" version="1.1" viewBox="0 0 56.693 56.693" width="56.693px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M40.43,21.739h-7.645v-5.014c0-1.883,1.248-2.322,2.127-2.322c0.877,0,5.395,0,5.395,0V6.125l-7.43-0.029  c-8.248,0-10.125,6.174-10.125,10.125v5.518h-4.77v8.53h4.77c0,10.947,0,24.137,0,24.137h10.033c0,0,0-13.32,0-24.137h6.77  L40.43,21.739z"/></svg>
    </a>
  

  
    <a id="scholar-link"
       class="icon" title="scholar" aria-label="Google Scholar"
       href="https://scholar.google.com/citations?user=AsgUcSEAAAAJ&hl=en&oi=ao">
      <?xml version="1.0" ?><!DOCTYPE svg  PUBLIC '-//W3C//DTD SVG 1.1//EN'  'http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd'><svg height="1755" width="1755" viewBox="0 0 1755 1755" xmlns="http://www.w3.org/2000/svg"><path transform="translate(0 1610) scale(1 -1)" d="M896.76 1130.189c-27.618 30.838-59.618 46.19-95.802 46.19-40.952 0-72.382-14.738-94.288-44.15-21.906-29.322-32.864-64.848-32.864-106.584 0-35.548 5.998-71.738 18-108.64 11.958-36.886 31.524-69.814 58.954-98.838 27.334-29.096 59.144-43.616 95.284-43.616 40.288 0 71.76 13.502 94.332 40.492 22.476 26.954 33.756 60.98 33.756 101.962 0 34.904-5.954 71.454-17.906 109.664-11.894 38.262-31.752 72.784-59.466 103.52zM1658.858 1512.573c-64.358 64.424-141.86 96.57-232.572 96.57h-1097.142c-90.712 0-168.14-32.146-232.572-96.57-64.424-64.286-96.57-141.86-96.57-232.572v-1097.142c0-90.712 32.146-168.288 96.57-232.712 64.432-64.146 142-96.432 232.572-96.432h1097.142c90.712 0 168.214 32.286 232.572 96.57 64.432 64.432 96.644 141.86 96.644 232.572v1097.142c0 90.712-32.22 168.288-96.644 232.572zM1297.81 1154.159v-392.126c0-18.154-14.856-33.016-33.016-33.016h-12.156c-18.162 0-33.016 14.856-33.016 33.016v392.126c0 16.12-2.34 29.578 20.188 32.41v52.172l-173.43-142.24c2.004-3.716 3.906-6.092 5.712-9.208 15.242-26.976 23.004-60.526 23.004-101.53 0-31.43-5.238-59.662-15.858-84.598-10.57-24.928-23.428-45.29-38.43-60.972-15.002-15.74-30.048-30.128-45.092-43.074-15.046-12.976-27.904-26.506-38.436-40.55-10.614-14-15.894-28.474-15.894-43.476 0-15.024 6.854-30.288 20.524-45.67 13.62-15.426 30.376-30.376 50.19-45.144 19.85-14.666 39.658-30.946 59.472-48.662 19.858-17.694 36.52-40.456 50.14-68.096 13.722-27.744 20.568-58.288 20.568-91.86 0-44.288-11.294-84.282-33.806-119.882-22.58-35.446-51.998-63.73-88.144-84.472-36.242-20.882-75-36.6-116.334-47.214-41.42-10.518-82.52-15.806-123.568-15.806-25.908 0-52.048 1.996-78.336 6.1-26.382 4.096-52.81 11.33-79.426 21.526-26.668 10.262-50.286 22.864-70.758 37.998-20.524 14.98-37.046 34.312-49.716 57.856-12.668 23.552-18.958 50.022-18.958 79.426 0 34.882 9.714 67.24 29.192 97.404 19.478 29.944 45.282 54.952 77.378 74.76 55.998 34.838 143.858 56.364 263.432 64.498-27.334 34.172-41.048 66.334-41.048 96.432 0 17.122 4.476 35.474 13.334 55.288-14.284-1.996-28.994-3.124-44.002-3.124-64.234 0-118.476 20.882-162.524 62.932-44.046 41.976-66.048 94.522-66.048 158.048 0 6.642 0.19 12.492 0.672 18.974h-261.046l393.618 342.17h651.856l-60.24-47.024v-82.996c22.368-2.874 20.004-16.318 20.004-32.394zM900.382 544.929c-7.52 1.36-18.088 2.122-31.708 2.122-29.382 0-58.288-2.596-86.666-7.782-28.38-5.046-56.378-13.568-83.998-25.592-27.722-11.952-50.096-29.528-67.146-52.766-17.144-23.208-25.666-50.542-25.666-81.994 0-29.974 7.52-56.714 22.572-80.004 15.002-23.142 34.808-41.26 59.428-54.236 24.62-12.998 50.432-22.814 77.378-29.264 26.998-6.408 54.476-9.736 82.476-9.736 55.376 0 103.050 12.47 143.046 37.406 39.906 24.928 59.904 63.422 59.904 115.382 0 10.928-1.522 21.686-4.528 32.19-3.138 10.62-6.24 19.712-9.282 27.26-3.050 7.41-8.858 16.332-17.43 26.616-8.522 10.314-15.046 17.934-19.434 23.004-4.476 5.238-12.852 12.712-25.19 22.594-12.236 9.926-20.048 16.114-23.522 18.402-3.43 2.406-12.332 8.908-26.668 19.456-14.328 10.634-22.184 16.274-23.566 16.94z" /></svg>

    </a>
  
  <!--<a id="subscribe-link"
     class="icon" title="Subscribe" aria-label="Subscribe"
     href="/feed.xml">
    <svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
    <path d="M0 0h24v24H0z" fill="none"/>
    <circle cx="6.18" cy="17.82" r="2.18"/>
    <path d="M4 4.44v2.83c7.03 0 12.73 5.7 12.73 12.73h2.83c0-8.59-6.97-15.56-15.56-15.56zm0 5.66v2.83c3.9 0 7.07 3.17 7.07 7.07h2.83c0-5.47-4.43-9.9-9.9-9.9z"/>
</svg>
  </a>

  
  
  
  

  
    <a id="tags-link"
       class="icon"
       title="Tags" aria-label="Tags"
       href="/tags.html">
      <svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
    <path d="M0 0h24v24H0z" fill="none"/>
    <path d="M17.63 5.84C17.27 5.33 16.67 5 16 5L5 5.01C3.9 5.01 3 5.9 3 7v10c0 1.1.9 1.99 2 1.99L16 19c.67 0 1.27-.33 1.63-.84L22 12l-4.37-6.16z"/>
</svg>
    </a>
  

  
    <a id="search-link"
       class="icon"
       title="Search" aria-label="Tags"
       href="/search.html">
      <svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
    <path d="M15.5 14h-.79l-.28-.27C15.41 12.59 16 11.11 16 9.5 16 5.91 13.09 3 9.5 3S3 5.91 3 9.5 5.91 16 9.5 16c1.61 0 3.09-.59 4.23-1.57l.27.28v.79l5 4.99L20.49 19l-4.99-5zm-6 0C7.01 14 5 11.99 5 9.5S7.01 5 9.5 5 14 7.01 14 9.5 11.99 14 9.5 14z"/>
    <path d="M0 0h24v24H0z" fill="none"/>
</svg>
    </a>
  -->

  <!-- Optional additional links to insert for icons links -->
</nav>
 </center>
  <p style="font-size:80%;">
  <!--(Site last updated: Mar, 2022)
  <br /> -->
  <a href="https://github.com/fongandrew/hydeout">Theme by Hydeout</a>
</p>

  </center>
</div>


    <main class="container">
      <header>
  <h1 class="page-title">Publications</h1>
</header>
<div class="content">
  <style>

table {
  margin-bottom: 1rem;
  width: 100%;
  font-size: 85%;
  border: 0px solid $border-color;
  border-collapse: collapse;
}

td,
th {
  padding:  1rem .25rem;
  border: 0px solid $border-color;
}

th {
  text-align: left;
}

tbody tr:nth-child(odd) td,
tbody tr:nth-child(odd) th {
  background-color: transparent;
}

paper {
 color: #; 
 font-weight:bold;
}


</style>

<h4 id="2022">2022</h4>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  
<tr>
       <td width="14%" valign="middle">
            <img src="/images/publications/web_SharedAutonomy.png" alt="HumanAI" style="vertical-align:middle; width: 80%; margin:0px 10px; border-radius:0%" /> 
     </td>
     <td valign="top" width="85%">
          <p>
              <paper>On Optimizing Interventions in Shared Autonomy</paper>
              <br /> 
              <a href="https://scholar.google.com/citations?user=-ccuMB4AAAAJ&amp;hl=zh-CN">Weihao Tan</a>*, 
              <a href="http://davidkoleczek.me/">David Koleczek</a>*, 
              <a href="https://scholar.google.com/citations?user=FzRyPc0AAAAJ&amp;hl=en">Siddhant Pradhan</a>*, 
              <a href="http://ds.cs.umass.edu/nicholas-perello">Nicholas Perello</a>, 
              <a href="https://www.linkedin.com/in/vivekchettiar/">Vivek Chettiar</a>, 
              <a href="https://yashchandak.github.io/publication/">Nan Ma</a>, 
              <a href="https://www.linkedin.com/in/aaslesha-rajaram/">Aaslesha Rajaram</a>, 
              <a href="https://www.linkedin.com/in/vishalrohra1/">Vishal Rohra</a>, 
              <a href="https://www.linkedin.com/in/soundararajansrinivasan/">Soundar Srinivasan</a>, 
              <a href="https://sites.google.com/view/sajjadriaj/">H M Sajjad Hossain</a>^, 
              <b>Yash Chandak</b>^.
            *Equal contribution, ^Equal advising
            <br />
              Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI 2022)
              <details>
                <summary>Abstract  | <a href="https://arxiv.org/pdf/2112.09169.pdf">Arxiv</a>  </summary>            
                  <p class="message">
                  Shared autonomy refers to approaches for enabling an autonomous agent to collaborate with a human with the aim of improving human performance. However, besides improving performance, it may often also be beneficial that the agent concurrently accounts for preserving the user’s experience or satisfaction of collaboration. In order to address this additional goal, we examine approaches for improving the user experience by constraining the number of interventions by the autonomous agent. We propose two model-free reinforcement learning methods that can account for both hard and soft constraints on the number of interventions. We show that not only does our method outperform the existing baseline, but also eliminates the need to manually tune a black-box hyperparameter for controlling the level of assistance. We also provide an in-depth analysis of intervention scenarios in order to further illuminate system understanding.
                  </p>
              </details>
          </p>  
     </td>
   </tr>
  
   <tr>
     <td width="14%" valign="middle">
            <img src="/images/publications/hopf.png" alt="HOPF" align="middle" style="width: 80%; margin:0px 10px; border-radius:0%" /> 
     </td>
     <td valign="top" width="85%">
          <p>
              <paper>Scaling Graph Propagation Kernels for Predictive Learning</paper>
              <br />
              <a href="https://priyeshv.github.io/">Priyesh Vijayan</a>
              <b>Yash Chandak</b>, 
              <a href="https://www.cse.iitm.ac.in/~miteshk/">Mitesh Khapra</a>,  
              <a href="http://web.cse.ohio-state.edu/~parthasarathy.2/">Srinivasan Parthasarathy</a>,    
              <a href="https://www.cse.iitm.ac.in/~ravi/">Balaraman Ravindran</a>
              <br />
              Frontiers in Big Data, section Data Mining and Management (Frontiers 2022).
              <details>
                <summary>Abstract </summary>            
                  <p class="message">
                     Given a graph where every node has certain attributes associated with it and some nodes have labels associated with them, Collective Classification (CC) is the task of assigning labels to every unlabeled node using information from the node as well as its neighbors. It is often the case that a node is not only influenced by its immediate neighbors but also by higher order neighbors, multiple hops away. Recent state-of-the-art models for CC learn end-to-end differentiable variations of Weisfeiler-Lehman (WL) kernels to aggregate multi-hop neighborhood information. In this work, we propose a Higher Order Propagation Framework, HOPF, which provides an iterative inference mechanism for these powerful differentiable kernels. Such a combination of classical iterative inference mechanism with recent differentiable kernels allows the framework to learn graph convolutional filters that simultaneously exploit the attribute and label information available in the neighborhood. Further, these iterative differentiable kernels can scale to larger hops beyond the memory limitations of existing differentiable kernels. We also show that existing WL kernel-based models suffer from the problem of Node Information Morphing where the information of the node is morphed or overwhelmed by the information of its neighbors when considering multiple hops. To address this, we propose a specific instantiation of HOPF, called the NIP models, which preserves the node information at every propagation step. The iterative formulation of NIP models further helps in incorporating distant hop information concisely as summaries of the inferred labels. We do an extensive evaluation across 11 datasets from different domains. We show that existing CC models do not provide consistent performance across datasets, while the proposed NIP model with iterative inference is more robust.
                  </p>
              </details>
          </p>  
     </td>
  </tr>  
</table>

<p><br /></p>

<h4 id="2021">2021</h4>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
<tr>
     <td width="14%" valign="middle">
            <img src="/images/publications/web_UnO.png" alt="UnO" style="vertical-align:middle; width: 80%; margin:0px 10px; border-radius:0%" /> 
     </td>
     <td valign="top" width="85%">
          <p>
              <paper>Universal Off-Policy Evaluation</paper>
              <br />
              <b>Yash Chandak</b>,  
              <a href="https://www.cs.utexas.edu/~sniekum/">Scott Niekum</a>,
              <a href="https://people.cs.umass.edu/~bsilva/">Bruno Castro da Silva</a>,
              <a href="https://people.cs.umass.edu/~elm/">Erik Learned-Miller</a>,
              <a href="https://cs.stanford.edu/people/ebrun/">Emma Brunskill</a>,
              <a href="https://people.cs.umass.edu/~pthomas/">Philip Thomas</a>
              <br />
             Thirty-fifth Conference on Neural Information Processing Systems (NeurIPS 2021)
              <details>
                <summary>Abstract  | <a href="https://arxiv.org/abs/2104.12820">Arxiv</a> |  <a href="https://github.com/yashchandak/UnO">Code</a> </summary>            
                  <p class="message">
                   When faced with sequential decision-making problems, it is often useful to be able to predict what would happen if decisions were made using a new policy. Those predictions must often be based on data collected under some previously used decision-making rule. Many previous methods enable such off-policy (or counterfactual) estimation of the expected value of a performance measure called the return. In this paper, we take the first steps towards a universal off-policy estimator (UnO) -- one that provides off-policy estimates and high-confidence bounds for any parameter of the return distribution. We use UnO for estimating and simultaneously bounding the mean, variance, quantiles/median, inter-quantile range, CVaR, and the entire cumulative distribution of returns. Finally, we also discuss Uno's applicability in various settings, including fully observable, partially observable (i.e., with unobserved confounders), Markovian, non-Markovian, stationary, smoothly non-stationary, and discrete distribution shifts. 
                  </p>
              </details>
          </p>  
     </td>
   </tr>
 
  <tr>
     <td width="14%" valign="middle">
            <img src="/images/publications/web_SOPE.png" alt="SOPE" style="vertical-align:middle; width: 80%; margin:0px 10px; border-radius:0%" /> 
     </td>
     <td valign="top" width="85%">
          <p>
              <paper>SOPE: Spectrum of Off-Policy Estimators</paper>
              <br />
              <a href="https://yashchandak.github.io/publication/">Christina Yuan</a>,
              <b>Yash Chandak</b>,  
              <a href="https://scholar.google.com/citations?user=PZPaJJ0AAAAJ&amp;hl=en">Stephen Giguere</a>,
              <a href="https://people.cs.umass.edu/~pthomas/">Philip Thomas</a>,
              <a href="https://www.cs.utexas.edu/~sniekum/">Scott Niekum</a>
              <br />
             Thirty-fifth Conference on Neural Information Processing Systems (NeurIPS 2021)
              <details>
                <summary>Abstract | <a href="https://arxiv.org/abs/2111.03936">Arxiv</a> |  <a href="https://github.com/Pearl-UTexas/SOPE">Code</a> </summary>            
                  <p class="message">
                   Many sequential decision making problems are high-stakes and require off-policy evaluation (OPE) of a new policy using historical data collected using some other policy. One of the most common OPE technique that provides unbiased estimates is trajectory based importance sampling (IS). However, due to the high variance of trajectory IS estimates, importance sampling methods based on stationary distributions (SIS) have recently been adopted. Unfortunately, while SIS often provides lower variance estimates, estimating the stationary distribution ratios can be challenging and lead to biased estimates. In this paper, we present a new perspective on this bias-variance trade-off and show the existence of a spectrum of estimators whose endpoints are SIS and IS, respectively. We then show that estimators in this spectrum allow us to trade-off between the bias and variance of IS and SIS and can achieve lower mean-squared error than both IS and SIS.
                  </p>
              </details>
          </p>  
     </td>
   </tr>
  
 <tr>
     <td width="14%" valign="middle">
            <img src="/images/publications/web_CVaR_BPG.png" alt="Risk_BPG" style="vertical-align:middle; width: 80%; margin:0px 10px; border-radius:0%" /> 
     </td>
     <td valign="top" width="85%">
          <p>
              <paper>Behavior Policy Search for Risk Estimators in Reinforcement Learning
</paper>
              <br />
              <a href="https://elitalobo.github.io/">Elita Lobo</a>,
              <b>Yash Chandak</b>,  
              <a href="https://scholar.google.com/citations?user=j54RzcEAAAAJ&amp;hl=en">Dharmashankar Subramanian</a>,
              <a href="https://pages.cs.wisc.edu/~jphanna/">Josiah Hanna</a>,
              <a href="https://marek.petrik.us/">Marek Petrik</a>
              <br />
             SafeRL workshop @ Thirty-fifth Conference on Neural Information Processing Systems (NeurIPS 2021)
              <details>
                <summary>Abstract | <a href="https://drive.google.com/file/d/1CVrXb7-exxJDSG3Ee2SxtezdtY79G8Nq/view">pdf</a>   </summary>            
                  <p class="message">
                   In real-world sequential decision problems, exploration is expensive, and the risk of expert decision policies must be evaluated from limited data. In this setting, Monte Carlo (MC) risk estimators are typically used to estimate the risk of decision policies. While these estimators have the desired low bias property, they often suffer from large variance. In this paper, we consider the problem of minimizing the asymptotic mean squared error and hence variance of MC risk estimators. 
We show that by carefully choosing the data sampling policy (\emph{behavior policy}), we can obtain low variance estimates of the risk of any given decision policy.
                  </p>
              </details>
          </p>  
     </td>
   </tr>
 
 
<tr>
       <td width="14%" valign="middle">
            <img src="/images/publications/web_HCGA.png" alt="HCGA" style="vertical-align:middle; width: 80%; margin:0px 10px; border-radius:0%" /> 
     </td>
     <td valign="top" width="85%">
          <p>
              <paper>High Confidence Generalization for Reinforcement Learning</paper>
              <br /> 
              <a href="https://people.cs.umass.edu/~jekostas/jekostas.html">James Kostas</a>, 
              <b>Yash Chandak</b>,  
              <a href="https://people.cs.umass.edu/~sjordan/">Scott Jordan</a>,
              <a href="https://research.adobe.com/person/georgios-theocharous/">Georgios Theocharous</a>, 
              <a href="https://people.cs.umass.edu/~pthomas/">Philip Thomas</a>
              <br />
              Thirty-eighth International Conference on Machine Learning (ICML 2021)
              <details>
                <summary>Abstract |  <a href="https://proceedings.mlr.press/v139/kostas21a.html">pdf</a>  </summary>            
                  <p class="message">
                   We present several classes of reinforcement learn-ing algorithms that safely generalize toMarkovdecision processes(MDPs) not seen during train-ing.  Specifically, we study the setting in whichsome set of MDPs is accessible for training. Forvarious definitions of safety, our algorithms giveprobabilistic guarantees that agents can safely gen-eralize to MDPs that are sampled from the samedistribution but are not necessarily in the train-ing set.   These algorithms are a type ofSeldo-nianalgorithm (Thomas et al., 2019), which is aclass of machine learning algorithms that returnmodels with probabilistic safety guarantees foruser-specified definitions of safety. 
                  </p>
              </details>
          </p>  
     </td>
   </tr>
  
<tr>
       <td width="14%" valign="middle">
            <img src="/images/publications/web_SharedAutonomy.png" alt="HumanAI" style="vertical-align:middle; width: 80%; margin:0px 10px; border-radius:0%" /> 
     </td>
     <td valign="top" width="85%">
          <p>
              <paper>Intervention Aware Shared Autonomy</paper>
              <br /> 
              <a href="https://scholar.google.com/citations?user=-ccuMB4AAAAJ&amp;hl=zh-CN">Weihao Tan</a>*, 
              <a href="http://davidkoleczek.me/">David Koleczek</a>*, 
              <a href="https://scholar.google.com/citations?user=FzRyPc0AAAAJ&amp;hl=en">Siddhant Pradhan</a>*, 
              <a href="http://ds.cs.umass.edu/nicholas-perello">Nicholas Perello</a>, 
              <a href="https://www.linkedin.com/in/vivekchettiar/">Vivek Chettiar</a>, 
              <a href="https://yashchandak.github.io/publication/">Nan Ma</a>, 
              <a href="https://www.linkedin.com/in/aaslesha-rajaram/">Aaslesha Rajaram</a>, 
              <a href="https://www.linkedin.com/in/vishalrohra1/">Vishal Rohra</a>, 
              <a href="https://www.linkedin.com/in/soundararajansrinivasan/">Soundar Srinivasan</a>, 
              <a href="https://sites.google.com/view/sajjadriaj/">H M Sajjad Hossain</a>^, 
              <b>Yash Chandak</b>^.
            *Equal contribution, ^Equal advising
            <br />
              HumanAI workshop @ Thirty-eighth International Conference on Machine Learning (ICML 2021)
              <details>
                <summary>Abstract |  <a href="https://all.cs.umass.edu/pubs/2021/Tan%20et%20al%20-%20Intervention%20Aware%20Shared%20Autonomy.pdf">pdf</a>  </summary>            
                  <p class="message">
                  Shared  autonomy  refers  to  approaches  for  en-abling an autonomous agent to collaborate witha human with the aim of improving human per-formance.  However, besides improving perfor-mance, it may often be beneficial that the agentconcurrently accounts for preserving the user’sexperience or satisfaction of collaboration. In or-der to address this additional goal, we examineapproaches for improving the user experience byconstraining the number of interventions by theautonomous agent. We propose two model-freereinforcement learning methods that can accountfor both hard and soft constraints on the numberof interventions. We show that not only does ourmethod outperform the existing baseline, but alsoeliminates the need to manually tune an arbitraryhyperparameter for controlling the level of assis-tance.  We also provide an in-depth analysis ofintervention scenarios in order to further illumi-nate system understanding.
                  </p>
              </details>
          </p>  
     </td>
   </tr>
  
  
<tr>
  <td width="14%" valign="middle">
            <img src="/images/publications/HCOVE.png" alt="SPIN" style="vertical-align:middle; width: 80%; margin:0px 10px; border-radius:0%" /> 
     </td>
     <td valign="top" width="85%">
          <p>
              <paper>High Confidence Off-Policy (or Counterfactual) Variance Estimation</paper>
              <br />
              <b>Yash Chandak</b>,  
              <a href="https://scholar.google.com/citations?user=yK56jugAAAAJ&amp;hl=en">Shiv Shankar</a>,
              <a href="https://people.cs.umass.edu/~pthomas/">Philip Thomas</a>
              <br />
              Thirty-fifth AAAI Conference on Artificial Intelligence (AAAI 2021)
              <details>
                <summary>Abstract  | <a href="https://arxiv.org/abs/2101.09847">Arxiv</a> |  <a href="https://github.com/yashchandak/UnO">Code</a> </summary>            
                  <p class="message">
                   Many sequential decision-making systems leverage data collected using prior policies to propose a new policy. In critical applications, it is important that high-confidence guarantees on the new policy's behavior are provided before deployment, to ensure that the policy will behave as desired. Prior works have studied high-confidence off-policy estimation of the \emph{expected} return, however, high-confidence off-policy estimation of the \emph{variance} of returns can be equally critical for high-risk applications. In this paper, we tackle the previously open problem of estimating and bounding, with high confidence, the variance of returns from off-policy data.
                  </p>
              </details>
          </p>  
     </td>
   </tr>

  
  
  
</table>

<p><br /></p>

<h4 id="2020">2020</h4>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
   <tr>
     <td width="14%" valign="middle">
            <img src="/images/publications/SPIN.png" alt="SPIN" style="vertical-align:middle; width: 80%; margin:0px 10px; border-radius:0%" /> 
     </td>
     <td valign="top" width="85%">
          <p>
              <paper>Towards Safe Policy Improvement for Non-Stationary MDPs</paper>
              <br />
              <b>Yash Chandak</b>,
              <a href="https://people.cs.umass.edu/~sjordan/">Scott Jordan</a>,
              <a href="https://research.adobe.com/person/georgios-theocharous/">Georgios Theocharous</a>,   
              <a href="https://webdocs.cs.ualberta.ca/~whitem/">Martha White</a>,   
              <a href="https://people.cs.umass.edu/~pthomas/">Philip Thomas</a>
              <br />
              <b>(Spotlight)</b> Thirty-fourth Conference on Neural Information Processing Systems (NeurIPS 2020)
              <details>
                <summary>Abstract | <a href="https://arxiv.org/abs/2010.12645">Arxiv</a> |  <a href="/blog/spin">Blogpost</a> |  <a href="https://github.com/ScottJordan/SafePolicyImprovementNonstationary">Code</a> | <a href="https://nips.cc/virtual/2020/public/poster_680390c55bbd9ce416d1d69a9ab4760d.html">Video</a>  </summary>            
                  <p class="message">
                    Many real-world sequential decision-making problems involve critical systems that present both human-life and financial risks. While several works in the past have proposed methods that are safe for deployment, they assume that the underlying problem is stationary. However, many real-world problems of interest exhibit non-stationarity, and when stakes are high, the cost associated with a false stationarity assumption may be unacceptable. Addressing safety in the presence of non-stationarity remains an open question in the literature. We present a type of Seldonian algorithm (Thomas et al., 2019), taking the first steps towards ensuring safety, with high confidence, for smoothly varying non-stationary decision problems, through a synthesis of model-free reinforcement learning algorithms with methods from time-series analysis.
                  </p>
              </details>
          </p>  
     </td>
   </tr>


   <tr>
     <td width="14%" valign="middle">
            <img src="/images/publications/prognosticator.png" alt="Future" style="vertical-align:middle; width: 80%; margin:0px 10px; border-radius:0%" /> 
     </td>
     <td valign="top" width="85%">
          <p>
              <paper>Optimizing for the Future in Non-Stationary MDPs</paper>
              <br />
              <b>Yash Chandak</b>, 
              <a href="https://research.adobe.com/person/georgios-theocharous/">Georgios Theocharous</a>,   
              <a href="https://scholar.google.com/citations?user=yK56jugAAAAJ&amp;hl=en">Shiv Shankar</a>,
              <a href="https://webdocs.cs.ualberta.ca/~whitem/">Martha White</a>,   
              <a href="https://people.cs.umass.edu/~mahadeva/Site/About_Me.html">Sridhar Mahadevan</a>,  
              <a href="https://people.cs.umass.edu/~pthomas/">Philip Thomas</a>
              <br />
              Thirty-seventh International Conference on Machine Learning (ICML 2020)
              <details>
                <summary>Abstract | <a href="https://arxiv.org/abs/2005.08158">Arxiv</a> |  <a href="/blog/prognosticator">Blogpost</a> |  <a href="https://github.com/yashchandak/OptFuture_NSMDP">Code</a> | <a href="https://icml.cc/virtual/2020/poster/6316">Video</a>  </summary>            
                  <p class="message">
                    Most reinforcement learning methods are based upon the key assumption that the transition dynamics and reward functions are fixed, that is, the underlying Markov decision process is stationary. However, in many real-world applications, this assumption is violated, and using existing algorithms may result in a performance lag. To proactively search for a good future policy, we present a policy gradient algorithm that maximizes a forecast of future performance. This forecast is obtained by fitting a curve to the counter-factual estimates of policy performance over time, without explicitly modeling the underlying non-stationarity. The resulting algorithm amounts to a non-uniform reweighting of past data, and we observe that minimizing performance over some of the data from past episodes can be beneficial when searching for a policy that maximizes future performance. We show that our algorithm, called Prognosticator, is more robust to non-stationarity than two online adaptation techniques, on three simulated problems motivated by real-world applications. 
                  </p>
              </details>
          </p>  
     </td>
   </tr>

   <tr>
     <td width="14%" valign="middle">
            <img src="/images/publications/evalrldm.png" alt="eval" style="vertical-align:middle; width: 80%; margin:0px 10px; border-radius:0%" /> 
     </td>
     <td valign="top" width="85%">
          <p>
              <paper>Evaluating the Performance of Reinforcement Learning Algorithms</paper>
              <br />
              <a href="https://people.cs.umass.edu/~sjordan/">Scott Jordan</a>,
              <b>Yash Chandak</b>,    
              <a href="https://people.cs.umass.edu/~dcohen/">Daniel Cohen</a>,  
              <a href="https://people.cs.umass.edu/~mengxuezhang/">Mengxue Zhang</a>,  
              <a href="https://people.cs.umass.edu/~pthomas/">Philip Thomas</a>
              <br />
              Thirty-seventh International Conference on Machine Learning (ICML 2020)
              <details>
                <summary>Abstract  | <a href="https://arxiv.org/abs/2006.16958">Arxiv</a> | <a href="https://github.com/ScottJordan/EvaluationOfRLAlgs">Code</a> | <a href="https://icml.cc/virtual/2020/poster/6301">Video</a>   </summary>            
                  <p class="message">
                    Performance evaluations are critical for quantifying algorithmic advances in reinforcement learning. Recent reproducibility analyses have shown that reported performance results are often inconsistent and difficult to replicate. In this work, we argue that the inconsistency of performance stems from the use of flawed evaluation metrics. Taking a step towards ensuring that reported results are consistent, we propose a new comprehensive evaluation methodology for reinforcement learning algorithms that produces reliable measurements of performance both on a single environment and when aggregated across environments. We demonstrate this method by evaluating a broad class of reinforcement learning algorithms on standard benchmark tasks. 
                  </p>
              </details>
          </p>  
     </td>
   </tr>



   <tr>
     <td width="14%" valign="middle">
            <img src="/images/publications/LAICA.png" alt="SAS" style="vertical-align:middle; width: 80%; margin:0px 10px; border-radius:0%" /> 
     </td>
     <td valign="top" width="85%">
          <p>
              <paper>Lifelong Learning with a Changing Action Set</paper>
              <br />
              <b>Yash Chandak</b>, 
              <a href="https://research.adobe.com/person/georgios-theocharous/">Georgios Theocharous</a>,   
              <a href="https://scholar.google.com/citations?user=clEBNJAAAAAJ&amp;hl=en">Chris Nota</a>, 
              <a href="https://people.cs.umass.edu/~pthomas/">Philip Thomas</a>
              <br />
              <b>(Oral)</b> Thirty-fourth AAAI Conference on Artificial Intelligence (AAAI 2020)
              <br />
              <font color="red">Outstanding Student Paper Honorable Mention.</font>
              <details>
                <summary>Abstract | <a href="https://arxiv.org/abs/1906.01770">Arxiv</a> | <a href="https://github.com/yashchandak/lifelong_changing_actions">Code</a> </summary>            
                  <p class="message">
                    In many real-world sequential decision making problems, the number of available actions (decisions) can vary over time. While problems like catastrophic forgetting, changing transition dynamics, changing rewards functions, etc. have been well-studied in the lifelong learning literature, the setting where the action set changes remains unaddressed. In this paper, we present an algorithm that autonomously adapts to an action set whose size changes over time. To tackle this open problem, we break it into two problems that can be solved iteratively: inferring the underlying, unknown, structure in the space of actions and optimizing a policy that leverages this structure. We demonstrate the efficiency of this approach on large-scale real-world lifelong learning problems. 
                  </p>
              </details>
          </p>  
     </td>
   </tr>

  <tr>
     <td width="14%" valign="middle">
            <img src="/images/publications/SAS.jpg" alt="SAS" style="vertical-align:middle; width: 80%; margin:0px 10px; border-radius:0%" /> 
     </td>
     <td valign="top" width="85%">
          <p>
              <paper>Reinforcement Learning When All Actions are Not Always Available</paper>
              <br />
              <b>Yash Chandak</b>, 
              <a href="https://research.adobe.com/person/georgios-theocharous/">Georgios Theocharous</a>,   
              <a href="https://bmetevier.github.io/">Blossom Metevier</a>, 
              <a href="https://people.cs.umass.edu/~pthomas/">Philip Thomas</a>
              <br />
              Thirty-fourth AAAI Conference on Artificial Intelligence (AAAI 2020)
              <details>
                <summary>Abstract | <a href="https://arxiv.org/abs/1906.01772">Arxiv</a> | <a href="https://github.com/yashchandak/SAS_RL">Code</a> </summary>            
                  <p class="message">
                   The Markov decision process (MDP) formulation used to model many real-world sequential decision making problems does not capture the setting where the set of available decisions (actions) at each time step is stochastic. Recently, the stochastic action set Markov decision process (SAS-MDP) formulation has been proposed, which captures the concept of a stochastic action set. In this paper we argue that existing RL algorithms for SAS-MDPs suffer from divergence issues, and present new algorithms for SAS-MDPs that incorporate variance reduction techniques unique to this setting, and provide conditions for their convergence. We conclude with experiments that demonstrate the practicality of our approaches using several tasks inspired by real-life use cases wherein the action set is stochastic. 
                  </p>
              </details>
          </p>  
     </td>
   </tr>


  <tr>
     <td width="14%" valign="middle">
            <img src="/images/publications/cogbias.png" alt="cogbias" style="vertical-align:middle; width: 80%; margin:0px 10px; border-radius:0%" /> 
     </td>
     <td valign="top" width="85%">
          <p>
              <paper>Reinforcement Learning for Strategic Recommendations</paper>
              <br />
              <a href="https://research.adobe.com/person/georgios-theocharous/">Georgios Theocharous</a>, 
              <b>Yash Chandak</b>,   
              <a href="https://people.cs.umass.edu/~pthomas/">Philip Thomas</a>,
              <a href="https://research.monash.edu/en/persons/frits-de-nijs">Frits de Nijs</a>
              <br />
              Technical Report
              <details>
                <summary>Abstract | <a href="https://arxiv.org/abs/2009.07346">Arxiv</a> </summary>            
                  <p class="message">
                   Strategic recommendations (SR) refer to the problem where an intelligent agent observes the sequential behaviors and activities of users and decides when and how to interact with them to optimize some long-term objectives, both for the user and the business. These systems are in their infancy in the industry and in need of practical solutions to some fundamental research challenges. At Adobe research, we have been implementing such systems for various use-cases, including points of interest recommendations, tutorial recommendations, next step guidance in multi-media editing software, and ad recommendation for optimizing lifetime value. There are many research challenges when building these systems, such as modeling the sequential behavior of users, deciding when to intervene and offer recommendations without annoying the user, evaluating policies offline with high confidence, safe deployment, non-stationarity, building systems from passive data that do not contain past recommendations, resource constraint optimization in multi-user systems, scaling to large and dynamic actions spaces, and handling and incorporating human cognitive biases. In this paper we cover various use-cases and research challenges we solved to make these systems practical. 
                  </p>
              </details>
          </p>  
     </td>
   </tr>


</table>

<p><br /></p>

<h4 id="2019">2019</h4>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  
   <tr>
     <td width="14%" valign="middle">
            <img src="/images/publications/action_rep.png" alt="action_representations" align="middle" style="width: 80%; margin:0px 10px; border-radius:0%" /> 
     </td>
     <td valign="top" width="85%">
          <p>
              <paper>Learning Action Representations for Reinforcement Learning</paper>
              <br />
              <b>Yash Chandak</b>, 
              <a href="https://research.adobe.com/person/georgios-theocharous/">Georgios Theocharous</a>,   
              <a href="https://people.cs.umass.edu/~jekostas/jekostas.html">James Kostas</a>, 
              <a href="https://people.cs.umass.edu/~sjordan/">Scott Jordan</a>,              
              <a href="https://people.cs.umass.edu/~pthomas/">Philip Thomas</a>
              <br />
              Thirty-sixth International Conference on Machine Learning (ICML 2019)
              <details>
                <summary>Abstract | <a href="https://arxiv.org/abs/1902.00183">Arxiv</a>  | <a href="https://slideslive.com/38917406/reinforcement-learning-theory?ref=speaker-18061-latest">Video</a> </summary>            
                  <p class="message">
                    Most model-free reinforcement learning methods leverage state representations (embeddings) for generalization, but either ignore structure in the space of actions or assume the structure is provided a priori. We show how a policy can be decomposed into a component that acts in a low-dimensional space of action representations and a component that transforms these representations into actual actions. These representations improve generalization over large, finite action sets by allowing the agent to infer the outcomes of actions similar to actions already taken. We provide an algorithm to both learn and use action representations and provide conditions for its convergence. The efficacy of the proposed method is demonstrated on large-scale real-world problems. 
                  </p>
              </details>
          </p>  
     </td>
   </tr>
   
   <tr>
     <td width="14%" valign="middle">
            <img src="/images/publications/action_rep_2.png" alt="action_generalization" align="middle" style="width: 80%; margin:0px 10px; border-radius:0%" /> 
     </td>
     <td valign="top" width="85%">
          <p>
              <paper>Improving Generalization over Large Action Sets</paper>
              <br />
              <b>Yash Chandak</b>, 
              <a href="https://research.adobe.com/person/georgios-theocharous/">Georgios Theocharous</a>,   
              <a href="https://people.cs.umass.edu/~jekostas/jekostas.html">James Kostas</a>, 
              <a href="https://people.cs.umass.edu/~sjordan/">Scott Jordan</a>,              
              <a href="https://people.cs.umass.edu/~pthomas/">Philip Thomas</a>
              <br />
              <b>(Oral)</b> 4th Multidisciplinary Conference on Reinforcement Learning and Decision Making (RLDM 2019)
          </p>  
     </td>
   </tr>  
   
   <tr>
     <td width="14%" valign="middle">
            <img src="/images/publications/eval.png" alt="eval" style="vertical-align:middle; width: 80%; margin:0px 10px; border-radius:0%" /> 
     </td>
     <td valign="top" width="85%">
          <p>
              <paper>Evaluating Reinforcement learning Algorithms Using Cumulative Distributions of Performance</paper>
              <br />
              <a href="https://people.cs.umass.edu/~sjordan/">Scott Jordan</a>,
              <b>Yash Chandak</b>,    
              <a href="https://people.cs.umass.edu/~mengxuezhang/">Mengxue Zhang</a>,  
              <a href="https://people.cs.umass.edu/~dcohen/">Daniel Cohen</a>,  
              <a href="https://people.cs.umass.edu/~pthomas/">Philip Thomas</a>
              <br />
              4th Multidisciplinary Conference on Reinforcement Learning and Decision Making (RLDM 2019)
          </p>  
     </td>
   </tr>
   
  <tr>
     <td width="14%" valign="middle">
            <img src="/images/publications/bellman_optimal.png" alt="Bellman" align="middle" style="width: 80%; margin:0px 10px; border-radius:0%" /> 
     </td>
     <td valign="top" width="85%">
          <p>
              <paper>Classical Policy Gradient: Preserving Bellman’s Principle of Optimality</paper>
              <br />
              <a href="https://people.cs.umass.edu/~pthomas/">Philip Thomas</a>,
              <a href="https://people.cs.umass.edu/~sjordan/">Scott Jordan</a>,                
              <b>Yash Chandak</b>,    
              <a href="https://scholar.google.com/citations?user=clEBNJAAAAAJ&amp;hl=en">Chris Nota</a>, 
              <a href="https://people.cs.umass.edu/~jekostas/jekostas.html">James Kostas</a>, 
              <br />
             Technical Report.
              <details>
                <summary>Abstract | <a href="https://arxiv.org/abs/1906.03063">Arxiv</a> </summary>            
                  <p class="message">
                    We propose a new objective function for finite-horizon episodic Markov decision processes that better captures Bellman's principle of optimality, and provide an expression for the gradient of the objective.  
                  </p>
              </details>
          </p>  
     </td>
   </tr>


</table>

<p><br /></p>

<h4 id="2018">2018</h4>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
   <tr>
     <td width="14%" valign="middle">
            <img src="/images/publications/dynamic_action.png" alt="dynamic_actions" align="middle" style="width: 80%; margin:0px 10px; border-radius:0%" /> 
     </td>
     <td valign="top" width="85%">
          <p>
              <paper>Reinforcement Learning with a Dynamic Action Set</paper>
              <br />
              <b>Yash Chandak</b>, 
              <a href="https://research.adobe.com/person/georgios-theocharous/">Georgios Theocharous</a>,   
              <a href="https://people.cs.umass.edu/~jekostas/jekostas.html">James Kostas</a>, 
              <a href="https://people.cs.umass.edu/~pthomas/">Philip Thomas</a>
              <br />
              Continual Learning workshop at the Thirty-second Conference on Neural Information Processing Systems (NeurIPS 2018)
          </p>  
     </td>
   </tr>

   <tr>
     <td width="14%" valign="middle">
            <img src="/images/publications/fgcn.png" alt="FGCN" align="middle" style="width: 80%; margin:0px 10px; border-radius:0%" /> 
     </td>
     <td valign="top" width="85%">
          <p>
              <paper>Fusion Graph Convolutional Networks</paper>
              <br />
              <a href="https://priyeshv.github.io/">Priyesh Vijayan</a>
              <b>Yash Chandak</b>, 
              <a href="https://www.cse.iitm.ac.in/~miteshk/">Mitesh Khapra</a>,   
              <a href="http://web.cse.ohio-state.edu/~parthasarathy.2/">Srinivasan Parthasarathy</a>, 
              <a href="https://www.cse.iitm.ac.in/~ravi/">Balaraman Ravindran</a>
              <br />
              14th International Workshop on Machine Learning with Graphs, 24th ACM SIGKDD Conference on
Knowledge Discovery and Data Mining (KDD 2018).
              <details>
                <summary>Abstract | <a href="https://arxiv.org/abs/1805.12528">Arxiv</a> | <a href="https://github.com/PriyeshV/HOPF">Code</a></summary>            
                  <p class="message">
                     Semi-supervised node classification in attributed graphs, i.e., graphs with node features, involves learning to classify unlabeled nodes given a partially labeled graph. Label predictions are made by jointly modeling the node and its' neighborhood features. State-of-the-art models for node classification on such attributed graphs use differentiable recursive functions that enable aggregation and filtering of neighborhood information from multiple hops. In this work, we analyze the representation capacity of these models to regulate information from multiple hops independently. From our analysis, we conclude that these models despite being powerful, have limited representation capacity to capture multi-hop neighborhood information effectively. Further, we also propose a mathematically motivated, yet simple extension to existing graph convolutional networks (GCNs) which has improved representation capacity. We extensively evaluate the proposed model, F-GCN on eight popular datasets from different domains. F-GCN outperforms the state-of-the-art models for semi-supervised learning on six datasets while being extremely competitive on the other two. 
                  </p>
              </details>
          </p>  
     </td>
   </tr>
      
   <tr>
     <td width="14%" valign="middle">
            <img src="/images/publications/hopf.png" alt="HOPF" align="middle" style="width: 80%; margin:0px 10px; border-radius:0%" /> 
     </td>
     <td valign="top" width="85%">
          <p>
              <paper>HOPF: Higher Order Propagation Framework for Deep Collective Classification</paper>
              <br />
              <a href="https://priyeshv.github.io/">Priyesh Vijayan</a>
              <b>Yash Chandak</b>, 
              <a href="https://www.cse.iitm.ac.in/~miteshk/">Mitesh Khapra</a>,  
              <a href="http://web.cse.ohio-state.edu/~parthasarathy.2/">Srinivasan Parthasarathy</a>,    
              <a href="https://www.cse.iitm.ac.in/~ravi/">Balaraman Ravindran</a>
              <br />
              Eighth International Workshop on Statistical Relational AI at the 27th International Joint Conference on
Artificial Intelligence (IJCAI 2018).
              <details>
                <summary>Abstract | <a href="https://arxiv.org/abs/1805.12421">Arxiv</a> | <a href="https://github.com/PriyeshV/HOPF">Code</a></summary>            
                  <p class="message">
                     Given a graph where every node has certain attributes associated with it and some nodes have labels associated with them, Collective Classification (CC) is the task of assigning labels to every unlabeled node using information from the node as well as its neighbors. It is often the case that a node is not only influenced by its immediate neighbors but also by higher order neighbors, multiple hops away. Recent state-of-the-art models for CC learn end-to-end differentiable variations of Weisfeiler-Lehman (WL) kernels to aggregate multi-hop neighborhood information. In this work, we propose a Higher Order Propagation Framework, HOPF, which provides an iterative inference mechanism for these powerful differentiable kernels. Such a combination of classical iterative inference mechanism with recent differentiable kernels allows the framework to learn graph convolutional filters that simultaneously exploit the attribute and label information available in the neighborhood. Further, these iterative differentiable kernels can scale to larger hops beyond the memory limitations of existing differentiable kernels. We also show that existing WL kernel-based models suffer from the problem of Node Information Morphing where the information of the node is morphed or overwhelmed by the information of its neighbors when considering multiple hops. To address this, we propose a specific instantiation of HOPF, called the NIP models, which preserves the node information at every propagation step. The iterative formulation of NIP models further helps in incorporating distant hop information concisely as summaries of the inferred labels. We do an extensive evaluation across 11 datasets from different domains. We show that existing CC models do not provide consistent performance across datasets, while the proposed NIP model with iterative inference is more robust.
                  </p>
              </details>
          </p>  
     </td>
   </tr>
   
</table>

<p><br /></p>

<h4 id="2015">2015</h4>

<table>
   <tr>
     <td width="14%" valign="middle">
            <img src="/images/publications/human_machine.png" alt="Human-Machine" align="middle" style="width: 80%; margin:0px 10px; border-radius:0%" /> 
     </td>
     <td valign="top" width="85%">
          <p>
              <paper>On Optimizing Human-Machine Task Assignment</paper>
              <br />
              <a href="https://www.cs.cornell.edu/~andreas/">Andreas Veit</a>,
              <a href="http://mjwilber.org/">Michael Wilber</a>,
              <a href="http://www.rajanvaish.com/index.html">Rajan Vaish</a>,
              <a href="https://tech.cornell.edu/people/serge-belongie/">Serge Belongie</a>,
              <a href="https://users.soe.ucsc.edu/~davis/">James Davis</a>,
              <b>Others</b>
              <br />
              The thrid AAAI Conference on Human Computation and Crowdsourcing (wip) (HCOMP 2015).
              <details>
                <summary>Abstract | <a href="https://arxiv.org/pdf/1509.07543.pdf">Arxiv</a> </summary>            
                  <p class="message">
                     When crowdsourcing systems are used in combination with machine inference systems in the real world, they benefit the most when the machine system is deeply integrated with the crowd workers. However, if researchers wish to integrate the crowd with "off-the-shelf" machine classifiers, this deep integration is not always possible. This work explores two strategies to increase accuracy and decrease cost under this setting. First, we show that reordering tasks presented to the human can create a significant accuracy improvement. Further, we show that greedily choosing parameters to maximize machine accuracy is sub-optimal, and joint optimization of the combined system improves performance. 
                  </p>
              </details>
          </p>  
     </td>
   </tr>
</table>

</div>

    </main>

    <!-- Optional footer content -->

    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-128250681-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-128250681-1');
</script>

  </body>
</html>
